# a standard robots exclusion file that excludes all
# but the most significant robots from visiting your site

# the default Disallow rule permits all of your web to be 
# crawled by the allowed agents, you will want to change this
# see http://info.webcrawler.com/mak/projects/robots/exclusion.html
# for further details

User-agent: *
Disallow: /Templates/
Disallow: /webstat/